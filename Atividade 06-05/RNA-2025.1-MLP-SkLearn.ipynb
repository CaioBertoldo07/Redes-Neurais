{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais Artificiais 2025.1\n",
    "\n",
    "\n",
    "## Implementação de Redes Neurais com Sci-Kit Learn\n",
    "\n",
    "* Professora: Elloá B. Guedes (ebgcosta@uea.edu.br)\n",
    "\n",
    "\n",
    "### Contexto: Consumo de Combustível\n",
    "\n",
    "O objetivo desta atividade prática é utilizar as ferramentas de Machine Learning no ambiente Python com o uso das bibliotecas pandas e sci-kit learn para prever o consumo de combustível de veículos.\n",
    "\n",
    "### Base de Dados\n",
    "\n",
    "Disponível em: https://archive.ics.uci.edu/ml/datasets/auto+mpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotecas\n",
    "\n",
    "Por hábito, a primeira célula do notebook costuma ser reservada para importação de bibliotecas.\n",
    "A cada biblioteca nova acrescida, é necessário executar a célula para atualização e correta execução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abertura do Dataset\n",
    "\n",
    "Abra o dataset e visualize o seu cabeçalho, isto é, os primeiros exemplos nele contidos.\n",
    "Isto é útil para checar se a importação foi realizada de maneira adequada e se a disposição dos dados está de acordo para os próximos passos do trabalho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
      "       'acceleration', 'modelyear', 'origin', 'name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('autompg.csv', sep=';')\n",
    "df.head()\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conhecendo o dataset\n",
    "\n",
    "Para praticar conceitos relativos à exploração do conjunto de dados, utilize as células a seguir para prover respostas para as seguintes perguntas:\n",
    "\n",
    "1. Quantos exemplos há no dataset?\n",
    "2. Quais os atributos existentes no dataset?\n",
    "3. Quais os nomes dos carros existentes no dataset?\n",
    "4. Quais as características do 'chevrolet camaro'?\n",
    "5. Qual a média de consumo, em galões por litro, dos carros existentes no dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Quantidade de exemplos no dataset: 406\n"
     ]
    }
   ],
   "source": [
    "numberOfExamples = len(df)\n",
    "print(f'1. Quantidade de exemplos no dataset: {numberOfExamples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Atributos existentes no dataset: ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'modelyear', 'origin', 'name']\n"
     ]
    }
   ],
   "source": [
    "attributes = df.columns.tolist()\n",
    "print(f'2. Atributos existentes no dataset: {attributes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Nomes dos carros existentes no dataset: ['chevrolet chevelle malibu' 'buick skylark 320' 'plymouth satellite'\n",
      " 'amc rebel sst' 'ford torino' 'ford galaxie 500' 'chevrolet impala'\n",
      " 'plymouth fury iii' 'pontiac catalina' 'amc ambassador dpl'\n",
      " 'citroen ds-21 pallas' 'chevrolet chevelle concours (sw)'\n",
      " 'ford torino (sw)' 'plymouth satellite (sw)' 'amc rebel sst (sw)'\n",
      " 'dodge challenger se' \"plymouth 'cuda 340\" 'ford mustang boss 302'\n",
      " 'chevrolet monte carlo' 'buick estate wagon (sw)' 'toyota corona mark ii'\n",
      " 'plymouth duster' 'amc hornet' 'ford maverick' 'datsun pl510'\n",
      " 'volkswagen 1131 deluxe sedan' 'peugeot 504' 'audi 100 ls' 'saab 99e'\n",
      " 'bmw 2002' 'amc gremlin' 'ford f250' 'chevy c20' 'dodge d200' 'hi 1200d'\n",
      " 'chevrolet vega 2300' 'toyota corona' 'ford pinto'\n",
      " 'volkswagen super beetle 117' 'plymouth satellite custom'\n",
      " 'ford torino 500' 'amc matador' 'pontiac catalina brougham'\n",
      " 'dodge monaco (sw)' 'ford country squire (sw)' 'pontiac safari (sw)'\n",
      " 'amc hornet sportabout (sw)' 'chevrolet vega (sw)' 'pontiac firebird'\n",
      " 'ford mustang' 'mercury capri 2000' 'opel 1900' 'peugeot 304' 'fiat 124b'\n",
      " 'toyota corolla 1200' 'datsun 1200' 'volkswagen model 111'\n",
      " 'plymouth cricket' 'toyota corona hardtop' 'dodge colt hardtop'\n",
      " 'volkswagen type 3' 'chevrolet vega' 'ford pinto runabout'\n",
      " 'amc ambassador sst' 'mercury marquis' 'buick lesabre custom'\n",
      " 'oldsmobile delta 88 royale' 'chrysler newport royal' 'mazda rx2 coupe'\n",
      " 'amc matador (sw)' 'ford gran torino (sw)'\n",
      " 'plymouth satellite custom (sw)' 'volvo 145e (sw)' 'volkswagen 411 (sw)'\n",
      " 'peugeot 504 (sw)' 'renault 12 (sw)' 'ford pinto (sw)' 'datsun 510 (sw)'\n",
      " 'toyouta corona mark ii (sw)' 'dodge colt (sw)'\n",
      " 'toyota corolla 1600 (sw)' 'buick century 350' 'chevrolet malibu'\n",
      " 'ford gran torino' 'dodge coronet custom' 'mercury marquis brougham'\n",
      " 'chevrolet caprice classic' 'ford ltd' 'plymouth fury gran sedan'\n",
      " 'chrysler new yorker brougham' 'buick electra 225 custom'\n",
      " 'amc ambassador brougham' 'plymouth valiant' 'chevrolet nova custom'\n",
      " 'volkswagen super beetle' 'ford country' 'plymouth custom suburb'\n",
      " 'oldsmobile vista cruiser' 'toyota carina' 'datsun 610' 'maxda rx3'\n",
      " 'mercury capri v6' 'fiat 124 sport coupe' 'chevrolet monte carlo s'\n",
      " 'pontiac grand prix' 'fiat 128' 'opel manta' 'audi 100ls' 'volvo 144ea'\n",
      " 'dodge dart custom' 'saab 99le' 'toyota mark ii' 'oldsmobile omega'\n",
      " 'chevrolet nova' 'datsun b210' 'chevrolet chevelle malibu classic'\n",
      " 'plymouth satellite sebring' 'buick century luxus (sw)'\n",
      " 'dodge coronet custom (sw)' 'audi fox' 'volkswagen dasher' 'datsun 710'\n",
      " 'dodge colt' 'fiat 124 tc' 'honda civic' 'subaru' 'fiat x1.9'\n",
      " 'plymouth valiant custom' 'mercury monarch' 'chevrolet bel air'\n",
      " 'plymouth grand fury' 'buick century' 'chevroelt chevelle malibu'\n",
      " 'plymouth fury' 'buick skyhawk' 'chevrolet monza 2+2' 'ford mustang ii'\n",
      " 'toyota corolla' 'pontiac astro' 'volkswagen rabbit' 'amc pacer'\n",
      " 'volvo 244dl' 'honda civic cvcc' 'fiat 131' 'capri ii' 'renault 12tl'\n",
      " 'dodge coronet brougham' 'chevrolet chevette' 'chevrolet woody'\n",
      " 'vw rabbit' 'dodge aspen se' 'ford granada ghia' 'pontiac ventura sj'\n",
      " 'amc pacer d/l' 'datsun b-210' 'volvo 245' 'plymouth volare premier v8'\n",
      " 'mercedes-benz 280s' 'cadillac seville' 'chevy c10' 'ford f108'\n",
      " 'dodge d100' 'honda accord cvcc' 'buick opel isuzu deluxe'\n",
      " 'renault 5 gtl' 'plymouth arrow gs' 'datsun f-10 hatchback'\n",
      " 'oldsmobile cutlass supreme' 'dodge monaco brougham'\n",
      " 'mercury cougar brougham' 'chevrolet concours' 'buick skylark'\n",
      " 'plymouth volare custom' 'ford granada' 'pontiac grand prix lj'\n",
      " 'chevrolet monte carlo landau' 'chrysler cordoba' 'ford thunderbird'\n",
      " 'volkswagen rabbit custom' 'pontiac sunbird coupe'\n",
      " 'toyota corolla liftback' 'ford mustang ii 2+2' 'dodge colt m/m'\n",
      " 'subaru dl' 'datsun 810' 'bmw 320i' 'mazda rx-4'\n",
      " 'volkswagen rabbit custom diesel' 'ford fiesta' 'mazda glc deluxe'\n",
      " 'datsun b210 gx' 'oldsmobile cutlass salon brougham' 'dodge diplomat'\n",
      " 'mercury monarch ghia' 'pontiac phoenix lj' 'ford fairmont (auto)'\n",
      " 'ford fairmont (man)' 'plymouth volare' 'amc concord'\n",
      " 'buick century special' 'mercury zephyr' 'dodge aspen' 'amc concord d/l'\n",
      " 'buick regal sport coupe (turbo)' 'ford futura' 'dodge magnum xe'\n",
      " 'datsun 510' 'dodge omni' 'toyota celica gt liftback' 'plymouth sapporo'\n",
      " 'oldsmobile starfire sx' 'datsun 200-sx' 'audi 5000' 'volvo 264gl'\n",
      " 'saab 99gle' 'peugeot 604sl' 'volkswagen scirocco' 'honda accord lx'\n",
      " 'pontiac lemans v6' 'mercury zephyr 6' 'ford fairmont 4'\n",
      " 'amc concord dl 6' 'dodge aspen 6' 'ford ltd landau'\n",
      " 'mercury grand marquis' 'dodge st. regis' 'chevrolet malibu classic (sw)'\n",
      " 'chrysler lebaron town @ country (sw)' 'vw rabbit custom'\n",
      " 'maxda glc deluxe' 'dodge colt hatchback custom' 'amc spirit dl'\n",
      " 'mercedes benz 300d' 'cadillac eldorado' 'plymouth horizon'\n",
      " 'plymouth horizon tc3' 'datsun 210' 'fiat strada custom'\n",
      " 'buick skylark limited' 'chevrolet citation' 'oldsmobile omega brougham'\n",
      " 'pontiac phoenix' 'toyota corolla tercel' 'datsun 310' 'ford fairmont'\n",
      " 'audi 4000' 'toyota corona liftback' 'mazda 626' 'datsun 510 hatchback'\n",
      " 'mazda glc' 'vw rabbit c (diesel)' 'vw dasher (diesel)'\n",
      " 'audi 5000s (diesel)' 'mercedes-benz 240d' 'honda civic 1500 gl'\n",
      " 'renault lecar deluxe' 'vokswagen rabbit' 'datsun 280-zx' 'mazda rx-7 gs'\n",
      " 'triumph tr7 coupe' 'ford mustang cobra' 'honda accord'\n",
      " 'plymouth reliant' 'dodge aries wagon (sw)' 'toyota starlet'\n",
      " 'plymouth champ' 'honda civic 1300' 'datsun 210 mpg' 'toyota tercel'\n",
      " 'mazda glc 4' 'plymouth horizon 4' 'ford escort 4w' 'ford escort 2h'\n",
      " 'volkswagen jetta' 'renault 18i' 'honda prelude' 'datsun 200sx'\n",
      " 'peugeot 505s turbo diesel' 'saab 900s' 'volvo diesel' 'toyota cressida'\n",
      " 'datsun 810 maxima' 'oldsmobile cutlass ls' 'ford granada gl'\n",
      " 'chrysler lebaron salon' 'chevrolet cavalier' 'chevrolet cavalier wagon'\n",
      " 'chevrolet cavalier 2-door' 'pontiac j2000 se hatchback' 'dodge aries se'\n",
      " 'ford fairmont futura' 'amc concord dl' 'volkswagen rabbit l'\n",
      " 'mazda glc custom l' 'mazda glc custom' 'plymouth horizon miser'\n",
      " 'mercury lynx l' 'nissan stanza xe' 'honda civic (auto)' 'datsun 310 gx'\n",
      " 'buick century limited' 'oldsmobile cutlass ciera (diesel)'\n",
      " 'chrysler lebaron medallion' 'ford granada l' 'toyota celica gt'\n",
      " 'dodge charger 2.2' 'chevrolet camaro' 'ford mustang gl' 'vw pickup'\n",
      " 'dodge rampage' 'ford ranger' 'chevy s-10']\n"
     ]
    }
   ],
   "source": [
    "carNames = df['name'].unique()\n",
    "print(f'3. Nomes dos carros existentes no dataset: {carNames}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
      "400  27.0        4.0         151.0        90.0  2950.0          17.3   \n",
      "\n",
      "     modelyear  origin              name  \n",
      "400       82.0     1.0  chevrolet camaro  \n"
     ]
    }
   ],
   "source": [
    "camaroFeatures = df[df['name'].str.contains('chevrolet camaro', case=False)]\n",
    "print(camaroFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média de consumo (km/l) dos carros no dataset: 10.00\n"
     ]
    }
   ],
   "source": [
    "df['km_per_liter'] = (df['mpg'] * 1.60934) / 3.78541\n",
    "print(f'Média de consumo (km/l) dos carros no dataset: {df['km_per_liter'].mean():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparação dos dados\n",
    "\n",
    "1. Existem exemplos com dados faltantes. Para fins de simplificação, elimine-os do dataset.\n",
    "2. Exclua a coluna com os nomes dos carros\n",
    "3. Converta mpg para km/l sabendo que: 1 mpg  = 0.425 km/l. Utilize apenas duas casas decimais nesta conversão.\n",
    "4. Remova a coluna mpg e insira a coluna kml no dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.replace('?', pd.NA, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df['horsepower'] = df['horsepower'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['name'], inplace=True)\n",
    "df.drop(columns=['km_per_liter'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df['kml'] = (df['mpg'] * 0.425).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>modelyear</th>\n",
       "      <th>origin</th>\n",
       "      <th>kml</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cylinders  displacement  horsepower  weight  acceleration  modelyear  \\\n",
       "0        8.0         307.0       130.0  3504.0          12.0       70.0   \n",
       "1        8.0         350.0       165.0  3693.0          11.5       70.0   \n",
       "2        8.0         318.0       150.0  3436.0          11.0       70.0   \n",
       "3        8.0         304.0       150.0  3433.0          12.0       70.0   \n",
       "4        8.0         302.0       140.0  3449.0          10.5       70.0   \n",
       "\n",
       "   origin   kml  \n",
       "0     1.0  7.65  \n",
       "1     1.0  6.38  \n",
       "2     1.0  7.65  \n",
       "3     1.0  6.80  \n",
       "4     1.0  7.22  "
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['mpg'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organização dos dados para treinamento\n",
    "\n",
    "1. Remova a coluna kml e atribua-a a uma variável Y\n",
    "2. Atribua os demais valores do dataset a uma variável X\n",
    "3. Efetue uma partição holdout 70/30 com o sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "Y = df['kml']\n",
    "X = df.drop(columns=['kml'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Necessário importar: from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (274, 7)\n",
      "X_test shape: (118, 7)\n",
      "Y_train shape: (274,)\n",
      "Y_test shape: (118,)\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'Y_train shape: {Y_train.shape}')\n",
    "print(f'Y_test shape: {Y_test.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento de um modelo de regressão linear\n",
    "\n",
    "1. Importe o modelo da biblioteca sklearn\n",
    "2. Instancie o modelo com parâmetros padrão (default)\n",
    "3. Execute o algoritmo de treinamento com os dados de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes do modelo: [-0.19602005  0.00809788 -0.00123014 -0.00300118  0.08239128  0.31182611\n",
      "  0.58545167]\n",
      "Intercepto: -7.361644654511377\n"
     ]
    }
   ],
   "source": [
    "regr = LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_train, Y_train)\n",
    "\n",
    "print(f'Coeficientes do modelo: {regr.coef_}')\n",
    "print(f'Intercepto: {regr.intercept_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste do modelo\n",
    "\n",
    "Vamos observar a saída do modelo para um exemplo individual existente nos dados de treino:\n",
    "* Atributos preditores: X_test[2:3]\n",
    "* Atributo alvo: Y_test.iloc[2]\n",
    "* Qual o resultado previsto para o modelo, dados estes atributos preditores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor real: 13.77\n",
      "Valor previsto: [13.98226376]\n"
     ]
    }
   ],
   "source": [
    "predictedValue = regr.predict(X_test)\n",
    "realValue = Y_test.iloc[2]\n",
    "\n",
    "print(f'Valor real: {realValue}')\n",
    "print(f'Valor previsto: {predictedValue[2:3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste do modelo\n",
    "\n",
    "1. Obtenha o R^2 para os dados de teste\n",
    " * Efetue a importação de r2_score do pacote sklearn.metrics\n",
    " * Trata-se de um valor no intervalo [0,1]\n",
    " * Quanto mais próximo de 1, melhor é o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_pred = regr.predict(X_test)\n",
    "r2 = r2_score(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² do modelo nos dados de teste: 0.8296210575226888\n"
     ]
    }
   ],
   "source": [
    "print(f'R² do modelo nos dados de teste: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtendo e visualizando os resíduos\n",
    "\n",
    "Uma maneira muito comum de visualizarmos o quão bom certo modelo é para aprender determinados padrões dá-se por meio da visualização dos resíduos, isto é, da diferença entre os valores previstos e observados. Adapte o código a seguir para calcular os resíduos produzidos pelo seu modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "residuos = []\n",
    "Y_predito = regr.predict(X_test)\n",
    "\n",
    "for (y_real ,y_pred) in zip(Y_test,Y_predito):\n",
    "    residuos.append((y_real - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [0,int(max(Y_test))]\n",
    "y = [0,0]\n",
    "\n",
    "plt.plot(x,y,linewidth=3)\n",
    "plt.plot(Y_test,residuos,'ro')\n",
    "plt.ylabel('Residuos (erro quadrático)')\n",
    "plt.xlabel('kml (valor real)')\n",
    "plt.title('Visualização dos Resíduos do Modelo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propondo RNAs MLP\n",
    "\n",
    "### Escalonando os atributos\n",
    "\n",
    "O treinamento de uma rede neural artificial é mais eficiente quando os valores que lhes são fornecidos como entrada são pequenos, pois isto favorece a convergência. Isto é feito escalonando todos os atributos para o intervalo [0,1], mas precisa ser feito de maneira cautelosa, para que informações do conjunto de teste não sejam fornecidas no treinamento.\n",
    "\n",
    "Há duas estratégias para tal escalonamento: normalização e padronização. Ambas possuem características particulares, vantagens e limitações, como é possível ver aqui: https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/\n",
    "\n",
    "No nosso caso, vamos usar a padronização. Assim, com os atributos preditores do treinamento, isto é, X_train, deve-se subtrair a média e dividir pelo desvio padrão:\n",
    "\n",
    "X_train_std = (X_train - np.mean(X_train))/np.std(X_train)\n",
    "\n",
    "Em seguida, o mesmo deve ser feito com os atributos preditores do conjunto de testes, mas com padronização relativa ao conjunto de treinamento:\n",
    "\n",
    "X_test_std = (X_test - np.mean(X_train))/np.std(X_train)\n",
    "\n",
    "Se todo o conjunto X for utilizado na padronização, a rede neural receberá informações do conjunto de teste por meio da média e variância utilizada para preparar os dados de treinamento, o que não é desejável.\n",
    "\n",
    "\n",
    "### Proposição de uma RNA MLP de Camada Única\n",
    "\n",
    "1. Consulte a documentação em https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "2. Treine uma rede neural multilayer perceptron para este problema com uma única camada e dez neurônios  \n",
    "    2.1 Utilize a função de ativação ReLU  \n",
    "    2.2 Utilize o solver Adam    \n",
    "    2.3 Imprima o passo a passo do treinamento    \n",
    "    2.4 Utilize o número máximo de épocas igual a 300\n",
    "3. Obtenha o $R^2$ do conjunto de testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 59.65564514\n",
      "Iteration 2, loss = 59.46532533\n",
      "Iteration 3, loss = 59.27843273\n",
      "Iteration 4, loss = 59.09629796\n",
      "Iteration 5, loss = 58.91177443\n",
      "Iteration 6, loss = 58.73271773\n",
      "Iteration 7, loss = 58.55547642\n",
      "Iteration 8, loss = 58.37900366\n",
      "Iteration 9, loss = 58.20964320\n",
      "Iteration 10, loss = 58.03494718\n",
      "Iteration 11, loss = 57.86851246\n",
      "Iteration 12, loss = 57.70041058\n",
      "Iteration 13, loss = 57.53446542\n",
      "Iteration 14, loss = 57.36978117\n",
      "Iteration 15, loss = 57.20862489\n",
      "Iteration 16, loss = 57.04576773\n",
      "Iteration 17, loss = 56.88621158\n",
      "Iteration 18, loss = 56.72553651\n",
      "Iteration 19, loss = 56.56975255\n",
      "Iteration 20, loss = 56.41225027\n",
      "Iteration 21, loss = 56.25494051\n",
      "Iteration 22, loss = 56.09716333\n",
      "Iteration 23, loss = 55.94346070\n",
      "Iteration 24, loss = 55.78931199\n",
      "Iteration 25, loss = 55.63232750\n",
      "Iteration 26, loss = 55.48200507\n",
      "Iteration 27, loss = 55.32651825\n",
      "Iteration 28, loss = 55.17585583\n",
      "Iteration 29, loss = 55.02450944\n",
      "Iteration 30, loss = 54.87069123\n",
      "Iteration 31, loss = 54.72184184\n",
      "Iteration 32, loss = 54.56978981\n",
      "Iteration 33, loss = 54.41991761\n",
      "Iteration 34, loss = 54.27024792\n",
      "Iteration 35, loss = 54.12013205\n",
      "Iteration 36, loss = 53.97141623\n",
      "Iteration 37, loss = 53.82316398\n",
      "Iteration 38, loss = 53.67507969\n",
      "Iteration 39, loss = 53.52527389\n",
      "Iteration 40, loss = 53.38007596\n",
      "Iteration 41, loss = 53.23216816\n",
      "Iteration 42, loss = 53.08847554\n",
      "Iteration 43, loss = 52.94064686\n",
      "Iteration 44, loss = 52.79519227\n",
      "Iteration 45, loss = 52.65296700\n",
      "Iteration 46, loss = 52.50853209\n",
      "Iteration 47, loss = 52.36339094\n",
      "Iteration 48, loss = 52.21680361\n",
      "Iteration 49, loss = 52.07499198\n",
      "Iteration 50, loss = 51.92718637\n",
      "Iteration 51, loss = 51.78262987\n",
      "Iteration 52, loss = 51.63735843\n",
      "Iteration 53, loss = 51.49126692\n",
      "Iteration 54, loss = 51.34609488\n",
      "Iteration 55, loss = 51.20164523\n",
      "Iteration 56, loss = 51.05601926\n",
      "Iteration 57, loss = 50.90870384\n",
      "Iteration 58, loss = 50.76412481\n",
      "Iteration 59, loss = 50.61927319\n",
      "Iteration 60, loss = 50.47427409\n",
      "Iteration 61, loss = 50.32846656\n",
      "Iteration 62, loss = 50.18273266\n",
      "Iteration 63, loss = 50.03910094\n",
      "Iteration 64, loss = 49.89353777\n",
      "Iteration 65, loss = 49.74662887\n",
      "Iteration 66, loss = 49.59942591\n",
      "Iteration 67, loss = 49.45076169\n",
      "Iteration 68, loss = 49.30104291\n",
      "Iteration 69, loss = 49.15336815\n",
      "Iteration 70, loss = 49.00162325\n",
      "Iteration 71, loss = 48.85114260\n",
      "Iteration 72, loss = 48.70026336\n",
      "Iteration 73, loss = 48.54643996\n",
      "Iteration 74, loss = 48.39253856\n",
      "Iteration 75, loss = 48.24132802\n",
      "Iteration 76, loss = 48.08883039\n",
      "Iteration 77, loss = 47.93405585\n",
      "Iteration 78, loss = 47.78117778\n",
      "Iteration 79, loss = 47.62897450\n",
      "Iteration 80, loss = 47.47249825\n",
      "Iteration 81, loss = 47.32153104\n",
      "Iteration 82, loss = 47.16686824\n",
      "Iteration 83, loss = 47.01130668\n",
      "Iteration 84, loss = 46.85733405\n",
      "Iteration 85, loss = 46.70199017\n",
      "Iteration 86, loss = 46.54610857\n",
      "Iteration 87, loss = 46.39196510\n",
      "Iteration 88, loss = 46.23486045\n",
      "Iteration 89, loss = 46.08176703\n",
      "Iteration 90, loss = 45.92460558\n",
      "Iteration 91, loss = 45.76677418\n",
      "Iteration 92, loss = 45.61165391\n",
      "Iteration 93, loss = 45.45456920\n",
      "Iteration 94, loss = 45.29793593\n",
      "Iteration 95, loss = 45.14073257\n",
      "Iteration 96, loss = 44.98085393\n",
      "Iteration 97, loss = 44.82281878\n",
      "Iteration 98, loss = 44.66648411\n",
      "Iteration 99, loss = 44.50578852\n",
      "Iteration 100, loss = 44.34823282\n",
      "Iteration 101, loss = 44.19194557\n",
      "Iteration 102, loss = 44.03150912\n",
      "Iteration 103, loss = 43.87362224\n",
      "Iteration 104, loss = 43.71491198\n",
      "Iteration 105, loss = 43.55621079\n",
      "Iteration 106, loss = 43.39871987\n",
      "Iteration 107, loss = 43.23715692\n",
      "Iteration 108, loss = 43.07798478\n",
      "Iteration 109, loss = 42.91913442\n",
      "Iteration 110, loss = 42.75835239\n",
      "Iteration 111, loss = 42.59721082\n",
      "Iteration 112, loss = 42.44007892\n",
      "Iteration 113, loss = 42.27885288\n",
      "Iteration 114, loss = 42.11791745\n",
      "Iteration 115, loss = 41.95412607\n",
      "Iteration 116, loss = 41.79862274\n",
      "Iteration 117, loss = 41.63141730\n",
      "Iteration 118, loss = 41.47139794\n",
      "Iteration 119, loss = 41.31050954\n",
      "Iteration 120, loss = 41.14831555\n",
      "Iteration 121, loss = 40.98580220\n",
      "Iteration 122, loss = 40.82236917\n",
      "Iteration 123, loss = 40.66199633\n",
      "Iteration 124, loss = 40.49809423\n",
      "Iteration 125, loss = 40.33273669\n",
      "Iteration 126, loss = 40.17091237\n",
      "Iteration 127, loss = 40.00708994\n",
      "Iteration 128, loss = 39.83973780\n",
      "Iteration 129, loss = 39.67875195\n",
      "Iteration 130, loss = 39.51078493\n",
      "Iteration 131, loss = 39.34415551\n",
      "Iteration 132, loss = 39.17864728\n",
      "Iteration 133, loss = 39.00902347\n",
      "Iteration 134, loss = 38.84141958\n",
      "Iteration 135, loss = 38.67146047\n",
      "Iteration 136, loss = 38.49926815\n",
      "Iteration 137, loss = 38.32668555\n",
      "Iteration 138, loss = 38.15454685\n",
      "Iteration 139, loss = 37.97433718\n",
      "Iteration 140, loss = 37.80025893\n",
      "Iteration 141, loss = 37.62484619\n",
      "Iteration 142, loss = 37.44704639\n",
      "Iteration 143, loss = 37.26654694\n",
      "Iteration 144, loss = 37.08927951\n",
      "Iteration 145, loss = 36.91060509\n",
      "Iteration 146, loss = 36.72928021\n",
      "Iteration 147, loss = 36.55462306\n",
      "Iteration 148, loss = 36.36856635\n",
      "Iteration 149, loss = 36.18791940\n",
      "Iteration 150, loss = 36.00397382\n",
      "Iteration 151, loss = 35.82313019\n",
      "Iteration 152, loss = 35.63734595\n",
      "Iteration 153, loss = 35.45653407\n",
      "Iteration 154, loss = 35.26646953\n",
      "Iteration 155, loss = 35.08338834\n",
      "Iteration 156, loss = 34.89694297\n",
      "Iteration 157, loss = 34.70921702\n",
      "Iteration 158, loss = 34.52251370\n",
      "Iteration 159, loss = 34.33426545\n",
      "Iteration 160, loss = 34.14501419\n",
      "Iteration 161, loss = 33.95494261\n",
      "Iteration 162, loss = 33.76092533\n",
      "Iteration 163, loss = 33.56592923\n",
      "Iteration 164, loss = 33.37771285\n",
      "Iteration 165, loss = 33.18401761\n",
      "Iteration 166, loss = 32.98532610\n",
      "Iteration 167, loss = 32.78504031\n",
      "Iteration 168, loss = 32.59075725\n",
      "Iteration 169, loss = 32.39534185\n",
      "Iteration 170, loss = 32.19982264\n",
      "Iteration 171, loss = 32.00031471\n",
      "Iteration 172, loss = 31.80450648\n",
      "Iteration 173, loss = 31.60974562\n",
      "Iteration 174, loss = 31.41189188\n",
      "Iteration 175, loss = 31.21667184\n",
      "Iteration 176, loss = 31.01692471\n",
      "Iteration 177, loss = 30.82336021\n",
      "Iteration 178, loss = 30.62183129\n",
      "Iteration 179, loss = 30.42792100\n",
      "Iteration 180, loss = 30.22743133\n",
      "Iteration 181, loss = 30.02602648\n",
      "Iteration 182, loss = 29.83010715\n",
      "Iteration 183, loss = 29.63053433\n",
      "Iteration 184, loss = 29.42516534\n",
      "Iteration 185, loss = 29.22516822\n",
      "Iteration 186, loss = 29.02463096\n",
      "Iteration 187, loss = 28.82557924\n",
      "Iteration 188, loss = 28.62359854\n",
      "Iteration 189, loss = 28.42444658\n",
      "Iteration 190, loss = 28.22021440\n",
      "Iteration 191, loss = 28.02113992\n",
      "Iteration 192, loss = 27.82367324\n",
      "Iteration 193, loss = 27.62300767\n",
      "Iteration 194, loss = 27.41885202\n",
      "Iteration 195, loss = 27.22195956\n",
      "Iteration 196, loss = 27.02552968\n",
      "Iteration 197, loss = 26.82206556\n",
      "Iteration 198, loss = 26.62712574\n",
      "Iteration 199, loss = 26.42264395\n",
      "Iteration 200, loss = 26.22991982\n",
      "Iteration 201, loss = 26.02917888\n",
      "Iteration 202, loss = 25.83056157\n",
      "Iteration 203, loss = 25.63351416\n",
      "Iteration 204, loss = 25.43564100\n",
      "Iteration 205, loss = 25.23807395\n",
      "Iteration 206, loss = 25.03927657\n",
      "Iteration 207, loss = 24.84103485\n",
      "Iteration 208, loss = 24.64365365\n",
      "Iteration 209, loss = 24.44628653\n",
      "Iteration 210, loss = 24.24719763\n",
      "Iteration 211, loss = 24.05480597\n",
      "Iteration 212, loss = 23.85755060\n",
      "Iteration 213, loss = 23.66348447\n",
      "Iteration 214, loss = 23.47109370\n",
      "Iteration 215, loss = 23.27891327\n",
      "Iteration 216, loss = 23.09067523\n",
      "Iteration 217, loss = 22.90044209\n",
      "Iteration 218, loss = 22.70838395\n",
      "Iteration 219, loss = 22.52388179\n",
      "Iteration 220, loss = 22.33429758\n",
      "Iteration 221, loss = 22.14873827\n",
      "Iteration 222, loss = 21.95689004\n",
      "Iteration 223, loss = 21.77300069\n",
      "Iteration 224, loss = 21.58134282\n",
      "Iteration 225, loss = 21.39474477\n",
      "Iteration 226, loss = 21.20969157\n",
      "Iteration 227, loss = 21.02337988\n",
      "Iteration 228, loss = 20.83319538\n",
      "Iteration 229, loss = 20.65138361\n",
      "Iteration 230, loss = 20.46615741\n",
      "Iteration 231, loss = 20.28054652\n",
      "Iteration 232, loss = 20.09838028\n",
      "Iteration 233, loss = 19.91682312\n",
      "Iteration 234, loss = 19.73623548\n",
      "Iteration 235, loss = 19.55563975\n",
      "Iteration 236, loss = 19.36951123\n",
      "Iteration 237, loss = 19.19598093\n",
      "Iteration 238, loss = 19.01633679\n",
      "Iteration 239, loss = 18.83789229\n",
      "Iteration 240, loss = 18.65967342\n",
      "Iteration 241, loss = 18.48623624\n",
      "Iteration 242, loss = 18.31117629\n",
      "Iteration 243, loss = 18.14069998\n",
      "Iteration 244, loss = 17.96199344\n",
      "Iteration 245, loss = 17.79152744\n",
      "Iteration 246, loss = 17.62055933\n",
      "Iteration 247, loss = 17.44959356\n",
      "Iteration 248, loss = 17.28109668\n",
      "Iteration 249, loss = 17.10917570\n",
      "Iteration 250, loss = 16.94597675\n",
      "Iteration 251, loss = 16.77602342\n",
      "Iteration 252, loss = 16.61170033\n",
      "Iteration 253, loss = 16.44751802\n",
      "Iteration 254, loss = 16.28045206\n",
      "Iteration 255, loss = 16.11983265\n",
      "Iteration 256, loss = 15.95691205\n",
      "Iteration 257, loss = 15.79626523\n",
      "Iteration 258, loss = 15.63227499\n",
      "Iteration 259, loss = 15.47574619\n",
      "Iteration 260, loss = 15.31411538\n",
      "Iteration 261, loss = 15.15549062\n",
      "Iteration 262, loss = 14.99799184\n",
      "Iteration 263, loss = 14.84126944\n",
      "Iteration 264, loss = 14.68590938\n",
      "Iteration 265, loss = 14.53075404\n",
      "Iteration 266, loss = 14.37781207\n",
      "Iteration 267, loss = 14.22345452\n",
      "Iteration 268, loss = 14.06864022\n",
      "Iteration 269, loss = 13.91832421\n",
      "Iteration 270, loss = 13.77131393\n",
      "Iteration 271, loss = 13.61965595\n",
      "Iteration 272, loss = 13.46905686\n",
      "Iteration 273, loss = 13.32695866\n",
      "Iteration 274, loss = 13.17757118\n",
      "Iteration 275, loss = 13.03182791\n",
      "Iteration 276, loss = 12.88919712\n",
      "Iteration 277, loss = 12.74881346\n",
      "Iteration 278, loss = 12.60471332\n",
      "Iteration 279, loss = 12.46515282\n",
      "Iteration 280, loss = 12.32668960\n",
      "Iteration 281, loss = 12.18684903\n",
      "Iteration 282, loss = 12.05107620\n",
      "Iteration 283, loss = 11.91326773\n",
      "Iteration 284, loss = 11.78392652\n",
      "Iteration 285, loss = 11.64463983\n",
      "Iteration 286, loss = 11.51382849\n",
      "Iteration 287, loss = 11.38411478\n",
      "Iteration 288, loss = 11.25484680\n",
      "Iteration 289, loss = 11.12547687\n",
      "Iteration 290, loss = 11.00065327\n",
      "Iteration 291, loss = 10.87291111\n",
      "Iteration 292, loss = 10.74991723\n",
      "Iteration 293, loss = 10.62881357\n",
      "Iteration 294, loss = 10.50780839\n",
      "Iteration 295, loss = 10.38869881\n",
      "Iteration 296, loss = 10.27350129\n",
      "Iteration 297, loss = 10.15345532\n",
      "Iteration 298, loss = 10.03788718\n",
      "Iteration 299, loss = 9.92350195\n",
      "Iteration 300, loss = 9.81036442\n",
      "R² da MLP no conjunto de testes: -0.5789321920956172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caiob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=(10,),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    verbose=True,\n",
    "    max_iter=300,\n",
    ")\n",
    "\n",
    "mlp.fit(X_train_std, Y_train)\n",
    "\n",
    "y_pred_mlp = mlp.predict(X_test_std)\n",
    "\n",
    "r2_mlp = r2_score(Y_test, y_pred_mlp)\n",
    "print(f'R² da MLP no conjunto de testes: {r2_mlp}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Proposição de uma RNA MLP com duas camadas ocultas\n",
    "\n",
    "1. Treine uma rede neural multilayer perceptron para este problema com duas camadas ocultas, com número de neurônios à sua escolha  \n",
    "    2.1 Utilize a função de ativação ReLU  \n",
    "    2.2 Utilize o solver Adam    \n",
    "    2.3 Imprima o passo a passo do treinamento    \n",
    "    2.4 Utilize o número máximo de épocas igual a 300\n",
    "2. Obtenha o $R^2$ do conjunto de testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 52.20770466\n",
      "Iteration 2, loss = 51.56353062\n",
      "Iteration 3, loss = 50.94388843\n",
      "Iteration 4, loss = 50.32642295\n",
      "Iteration 5, loss = 49.72090501\n",
      "Iteration 6, loss = 49.12872177\n",
      "Iteration 7, loss = 48.54911966\n",
      "Iteration 8, loss = 47.96414716\n",
      "Iteration 9, loss = 47.39630431\n",
      "Iteration 10, loss = 46.84620322\n",
      "Iteration 11, loss = 46.30188384\n",
      "Iteration 12, loss = 45.75870436\n",
      "Iteration 13, loss = 45.22998911\n",
      "Iteration 14, loss = 44.70150349\n",
      "Iteration 15, loss = 44.18261950\n",
      "Iteration 16, loss = 43.66431586\n",
      "Iteration 17, loss = 43.16468794\n",
      "Iteration 18, loss = 42.65025148\n",
      "Iteration 19, loss = 42.15476838\n",
      "Iteration 20, loss = 41.65853770\n",
      "Iteration 21, loss = 41.16019420\n",
      "Iteration 22, loss = 40.65780967\n",
      "Iteration 23, loss = 40.17708222\n",
      "Iteration 24, loss = 39.67246192\n",
      "Iteration 25, loss = 39.18274999\n",
      "Iteration 26, loss = 38.68854439\n",
      "Iteration 27, loss = 38.20002278\n",
      "Iteration 28, loss = 37.70476436\n",
      "Iteration 29, loss = 37.21444852\n",
      "Iteration 30, loss = 36.70855526\n",
      "Iteration 31, loss = 36.22581757\n",
      "Iteration 32, loss = 35.72477176\n",
      "Iteration 33, loss = 35.23249288\n",
      "Iteration 34, loss = 34.72526476\n",
      "Iteration 35, loss = 34.22834399\n",
      "Iteration 36, loss = 33.72533042\n",
      "Iteration 37, loss = 33.22045927\n",
      "Iteration 38, loss = 32.71531039\n",
      "Iteration 39, loss = 32.19746473\n",
      "Iteration 40, loss = 31.68700909\n",
      "Iteration 41, loss = 31.17694543\n",
      "Iteration 42, loss = 30.65313690\n",
      "Iteration 43, loss = 30.14247791\n",
      "Iteration 44, loss = 29.62194750\n",
      "Iteration 45, loss = 29.08523886\n",
      "Iteration 46, loss = 28.56422797\n",
      "Iteration 47, loss = 28.04293431\n",
      "Iteration 48, loss = 27.50810975\n",
      "Iteration 49, loss = 26.97595085\n",
      "Iteration 50, loss = 26.43815133\n",
      "Iteration 51, loss = 25.89991084\n",
      "Iteration 52, loss = 25.35877864\n",
      "Iteration 53, loss = 24.80820933\n",
      "Iteration 54, loss = 24.26354650\n",
      "Iteration 55, loss = 23.70775289\n",
      "Iteration 56, loss = 23.15207496\n",
      "Iteration 57, loss = 22.60416209\n",
      "Iteration 58, loss = 22.03893132\n",
      "Iteration 59, loss = 21.48654975\n",
      "Iteration 60, loss = 20.92096926\n",
      "Iteration 61, loss = 20.36347007\n",
      "Iteration 62, loss = 19.79602616\n",
      "Iteration 63, loss = 19.24905381\n",
      "Iteration 64, loss = 18.68293808\n",
      "Iteration 65, loss = 18.12471070\n",
      "Iteration 66, loss = 17.57171101\n",
      "Iteration 67, loss = 17.02055799\n",
      "Iteration 68, loss = 16.46641689\n",
      "Iteration 69, loss = 15.92963180\n",
      "Iteration 70, loss = 15.39251199\n",
      "Iteration 71, loss = 14.85096659\n",
      "Iteration 72, loss = 14.31816862\n",
      "Iteration 73, loss = 13.80727286\n",
      "Iteration 74, loss = 13.28450735\n",
      "Iteration 75, loss = 12.76823972\n",
      "Iteration 76, loss = 12.27886923\n",
      "Iteration 77, loss = 11.77947247\n",
      "Iteration 78, loss = 11.29229630\n",
      "Iteration 79, loss = 10.82405436\n",
      "Iteration 80, loss = 10.35574811\n",
      "Iteration 81, loss = 9.90673505\n",
      "Iteration 82, loss = 9.46591604\n",
      "Iteration 83, loss = 9.03795014\n",
      "Iteration 84, loss = 8.62745255\n",
      "Iteration 85, loss = 8.22415724\n",
      "Iteration 86, loss = 7.83383444\n",
      "Iteration 87, loss = 7.46875618\n",
      "Iteration 88, loss = 7.11347640\n",
      "Iteration 89, loss = 6.76242198\n",
      "Iteration 90, loss = 6.43847094\n",
      "Iteration 91, loss = 6.12714942\n",
      "Iteration 92, loss = 5.83713879\n",
      "Iteration 93, loss = 5.54810299\n",
      "Iteration 94, loss = 5.28857107\n",
      "Iteration 95, loss = 5.03749666\n",
      "Iteration 96, loss = 4.80176602\n",
      "Iteration 97, loss = 4.58549676\n",
      "Iteration 98, loss = 4.37335880\n",
      "Iteration 99, loss = 4.18132878\n",
      "Iteration 100, loss = 4.00407603\n",
      "Iteration 101, loss = 3.83653189\n",
      "Iteration 102, loss = 3.68265043\n",
      "Iteration 103, loss = 3.53876971\n",
      "Iteration 104, loss = 3.40801033\n",
      "Iteration 105, loss = 3.28904161\n",
      "Iteration 106, loss = 3.17554248\n",
      "Iteration 107, loss = 3.07577939\n",
      "Iteration 108, loss = 2.98219639\n",
      "Iteration 109, loss = 2.89516015\n",
      "Iteration 110, loss = 2.82102592\n",
      "Iteration 111, loss = 2.74972431\n",
      "Iteration 112, loss = 2.68151318\n",
      "Iteration 113, loss = 2.62243211\n",
      "Iteration 114, loss = 2.56650802\n",
      "Iteration 115, loss = 2.51776685\n",
      "Iteration 116, loss = 2.47135529\n",
      "Iteration 117, loss = 2.42827814\n",
      "Iteration 118, loss = 2.38971267\n",
      "Iteration 119, loss = 2.35218385\n",
      "Iteration 120, loss = 2.31873707\n",
      "Iteration 121, loss = 2.28716024\n",
      "Iteration 122, loss = 2.25667019\n",
      "Iteration 123, loss = 2.22922032\n",
      "Iteration 124, loss = 2.20342294\n",
      "Iteration 125, loss = 2.17673796\n",
      "Iteration 126, loss = 2.15288781\n",
      "Iteration 127, loss = 2.13034356\n",
      "Iteration 128, loss = 2.10669636\n",
      "Iteration 129, loss = 2.08598661\n",
      "Iteration 130, loss = 2.06360246\n",
      "Iteration 131, loss = 2.04423806\n",
      "Iteration 132, loss = 2.02419010\n",
      "Iteration 133, loss = 2.00479028\n",
      "Iteration 134, loss = 1.98584340\n",
      "Iteration 135, loss = 1.96754835\n",
      "Iteration 136, loss = 1.95025694\n",
      "Iteration 137, loss = 1.93287230\n",
      "Iteration 138, loss = 1.91519299\n",
      "Iteration 139, loss = 1.89805394\n",
      "Iteration 140, loss = 1.88267593\n",
      "Iteration 141, loss = 1.86609334\n",
      "Iteration 142, loss = 1.85121479\n",
      "Iteration 143, loss = 1.83492509\n",
      "Iteration 144, loss = 1.82073318\n",
      "Iteration 145, loss = 1.80579993\n",
      "Iteration 146, loss = 1.79121727\n",
      "Iteration 147, loss = 1.77799076\n",
      "Iteration 148, loss = 1.76440613\n",
      "Iteration 149, loss = 1.75109654\n",
      "Iteration 150, loss = 1.73759057\n",
      "Iteration 151, loss = 1.72538047\n",
      "Iteration 152, loss = 1.71254146\n",
      "Iteration 153, loss = 1.70025912\n",
      "Iteration 154, loss = 1.68835288\n",
      "Iteration 155, loss = 1.67634768\n",
      "Iteration 156, loss = 1.66468919\n",
      "Iteration 157, loss = 1.65353491\n",
      "Iteration 158, loss = 1.64150582\n",
      "Iteration 159, loss = 1.63089268\n",
      "Iteration 160, loss = 1.62021792\n",
      "Iteration 161, loss = 1.60975126\n",
      "Iteration 162, loss = 1.59950555\n",
      "Iteration 163, loss = 1.58953217\n",
      "Iteration 164, loss = 1.57976590\n",
      "Iteration 165, loss = 1.57004604\n",
      "Iteration 166, loss = 1.55997509\n",
      "Iteration 167, loss = 1.55125903\n",
      "Iteration 168, loss = 1.54191957\n",
      "Iteration 169, loss = 1.53299987\n",
      "Iteration 170, loss = 1.52372435\n",
      "Iteration 171, loss = 1.51495190\n",
      "Iteration 172, loss = 1.50655278\n",
      "Iteration 173, loss = 1.49785154\n",
      "Iteration 174, loss = 1.48969449\n",
      "Iteration 175, loss = 1.48197168\n",
      "Iteration 176, loss = 1.47370567\n",
      "Iteration 177, loss = 1.46650948\n",
      "Iteration 178, loss = 1.45868692\n",
      "Iteration 179, loss = 1.45120649\n",
      "Iteration 180, loss = 1.44427553\n",
      "Iteration 181, loss = 1.43746741\n",
      "Iteration 182, loss = 1.43003493\n",
      "Iteration 183, loss = 1.42335080\n",
      "Iteration 184, loss = 1.41631868\n",
      "Iteration 185, loss = 1.40935288\n",
      "Iteration 186, loss = 1.40317176\n",
      "Iteration 187, loss = 1.39656328\n",
      "Iteration 188, loss = 1.39038296\n",
      "Iteration 189, loss = 1.38445279\n",
      "Iteration 190, loss = 1.37808303\n",
      "Iteration 191, loss = 1.37278736\n",
      "Iteration 192, loss = 1.36644025\n",
      "Iteration 193, loss = 1.36125118\n",
      "Iteration 194, loss = 1.35569437\n",
      "Iteration 195, loss = 1.35033319\n",
      "Iteration 196, loss = 1.34492212\n",
      "Iteration 197, loss = 1.33978212\n",
      "Iteration 198, loss = 1.33504715\n",
      "Iteration 199, loss = 1.32978886\n",
      "Iteration 200, loss = 1.32481923\n",
      "Iteration 201, loss = 1.32000543\n",
      "Iteration 202, loss = 1.31518558\n",
      "Iteration 203, loss = 1.31081604\n",
      "Iteration 204, loss = 1.30585068\n",
      "Iteration 205, loss = 1.30147661\n",
      "Iteration 206, loss = 1.29684921\n",
      "Iteration 207, loss = 1.29194669\n",
      "Iteration 208, loss = 1.28715601\n",
      "Iteration 209, loss = 1.28303630\n",
      "Iteration 210, loss = 1.27840036\n",
      "Iteration 211, loss = 1.27395819\n",
      "Iteration 212, loss = 1.26920930\n",
      "Iteration 213, loss = 1.26503226\n",
      "Iteration 214, loss = 1.26049514\n",
      "Iteration 215, loss = 1.25632271\n",
      "Iteration 216, loss = 1.25196931\n",
      "Iteration 217, loss = 1.24766836\n",
      "Iteration 218, loss = 1.24374242\n",
      "Iteration 219, loss = 1.23943699\n",
      "Iteration 220, loss = 1.23542203\n",
      "Iteration 221, loss = 1.23142932\n",
      "Iteration 222, loss = 1.22742318\n",
      "Iteration 223, loss = 1.22351865\n",
      "Iteration 224, loss = 1.21977610\n",
      "Iteration 225, loss = 1.21573854\n",
      "Iteration 226, loss = 1.21204249\n",
      "Iteration 227, loss = 1.20865040\n",
      "Iteration 228, loss = 1.20496805\n",
      "Iteration 229, loss = 1.20157462\n",
      "Iteration 230, loss = 1.19761364\n",
      "Iteration 231, loss = 1.19472118\n",
      "Iteration 232, loss = 1.19091664\n",
      "Iteration 233, loss = 1.18743046\n",
      "Iteration 234, loss = 1.18442040\n",
      "Iteration 235, loss = 1.18086545\n",
      "Iteration 236, loss = 1.17755372\n",
      "Iteration 237, loss = 1.17441257\n",
      "Iteration 238, loss = 1.17104701\n",
      "Iteration 239, loss = 1.16783411\n",
      "Iteration 240, loss = 1.16445905\n",
      "Iteration 241, loss = 1.16140828\n",
      "Iteration 242, loss = 1.15797977\n",
      "Iteration 243, loss = 1.15477540\n",
      "Iteration 244, loss = 1.15158488\n",
      "Iteration 245, loss = 1.14827990\n",
      "Iteration 246, loss = 1.14553375\n",
      "Iteration 247, loss = 1.14211180\n",
      "Iteration 248, loss = 1.13934099\n",
      "Iteration 249, loss = 1.13629040\n",
      "Iteration 250, loss = 1.13324486\n",
      "Iteration 251, loss = 1.13033970\n",
      "Iteration 252, loss = 1.12738598\n",
      "Iteration 253, loss = 1.12443243\n",
      "Iteration 254, loss = 1.12182860\n",
      "Iteration 255, loss = 1.11863701\n",
      "Iteration 256, loss = 1.11547429\n",
      "Iteration 257, loss = 1.11288327\n",
      "Iteration 258, loss = 1.11005232\n",
      "Iteration 259, loss = 1.10730016\n",
      "Iteration 260, loss = 1.10457481\n",
      "Iteration 261, loss = 1.10201948\n",
      "Iteration 262, loss = 1.09943889\n",
      "Iteration 263, loss = 1.09697200\n",
      "Iteration 264, loss = 1.09445837\n",
      "Iteration 265, loss = 1.09182121\n",
      "Iteration 266, loss = 1.08941451\n",
      "Iteration 267, loss = 1.08693060\n",
      "Iteration 268, loss = 1.08458969\n",
      "Iteration 269, loss = 1.08243596\n",
      "Iteration 270, loss = 1.07993967\n",
      "Iteration 271, loss = 1.07740568\n",
      "Iteration 272, loss = 1.07520967\n",
      "Iteration 273, loss = 1.07251676\n",
      "Iteration 274, loss = 1.07028673\n",
      "Iteration 275, loss = 1.06799187\n",
      "Iteration 276, loss = 1.06540508\n",
      "Iteration 277, loss = 1.06333469\n",
      "Iteration 278, loss = 1.06063158\n",
      "Iteration 279, loss = 1.05836587\n",
      "Iteration 280, loss = 1.05594336\n",
      "Iteration 281, loss = 1.05362644\n",
      "Iteration 282, loss = 1.05135163\n",
      "Iteration 283, loss = 1.04910137\n",
      "Iteration 284, loss = 1.04683698\n",
      "Iteration 285, loss = 1.04465639\n",
      "Iteration 286, loss = 1.04225556\n",
      "Iteration 287, loss = 1.04004443\n",
      "Iteration 288, loss = 1.03764571\n",
      "Iteration 289, loss = 1.03540547\n",
      "Iteration 290, loss = 1.03316041\n",
      "Iteration 291, loss = 1.03083204\n",
      "Iteration 292, loss = 1.02863345\n",
      "Iteration 293, loss = 1.02648703\n",
      "Iteration 294, loss = 1.02417311\n",
      "Iteration 295, loss = 1.02194800\n",
      "Iteration 296, loss = 1.01958446\n",
      "Iteration 297, loss = 1.01751563\n",
      "Iteration 298, loss = 1.01551161\n",
      "Iteration 299, loss = 1.01299504\n",
      "Iteration 300, loss = 1.01110125\n",
      "R² da MLP com duas camadas ocultas no conjunto de testes: 0.8125139914074272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caiob\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlp2 = MLPRegressor(\n",
    "    hidden_layer_sizes=(20, 10),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    verbose=True,\n",
    "    max_iter=300,\n",
    ")\n",
    "\n",
    "mlp2.fit(X_train_std, Y_train)\n",
    "\n",
    "y_pred_mlp2 = mlp2.predict(X_test_std)\n",
    "\n",
    "r2_mlp2 = r2_score(Y_test, y_pred_mlp2)\n",
    "print(f'R² da MLP com duas camadas ocultas no conjunto de testes: {r2_mlp2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para Discussão\n",
    "\n",
    "- Qual melhor modelo para este problema?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acredito que depende um pouco, por exemplo, nesse problema, como o ganho de R² não é muito alto, o modelo com uma camada ou até mesmo a regressão linewar pode ser melhor por ser mais simples e rápido, mas se o Mlp com 2 camadas obtiver um R² significativamente melhor no teste sem overfitting, ele é o melhor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
